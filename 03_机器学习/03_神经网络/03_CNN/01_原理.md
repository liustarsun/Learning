# 1. 卷积神经网络
1. 分为几层？
2. 每层用来干嘛？
3. 用了什么样的原理？
4. 有没有缺陷？

# 2. 卷积层
1. 通道：在图片识别中，**如果每个像素只表示一种颜色的强度，则通道就是1，如果是用R，G，B表示，则通道就是3，把3个通道的像素矩阵叠加在一起，就是一个彩色的图，可以类比为三位空间的z轴的概念**
2. 在图像处理中，卷积操作主要是利用特征模板**对原始信号进行滤波操作**，从而达到特征提取的目的
> 原理：通过从输入的一个小块数据矩阵(也就是一小块图像中)学习到图像的特征，并且能保留像素间的相对空间关系===>**这是如何做到的？**

## 2.1 卷积核
1. 利用一个局部区域去扫描整个图像，在这个局部区域的作用下，**图像中的像素点都会被线性变换组合，形成下一层的神经元节点**===>**这个区域就称为卷积核**==>某些场合，也称为**滤波器或特征检测器**
2. 卷积操作即矩阵的点乘操作，使用卷积核对图像按照**从左到右，从上到下做卷积操作**，可以得到图像的特征图谱===>也称**卷积特征或激活图**
3. 离散卷积是一个线性操作，因此这种卷积操作也被称为线性滤波==>所谓的线性，是指**使用每个像素的邻域的线性组合来代替这个像素**
4. 卷积操作**保留了图像的空间特征**，由于空间是共享的，因此使用同一卷积核，在不同位置的同一物体的形状都可以被识别，而不需要针对每个位置都进行学习
> 不太理解？？

## 2.2 使用卷积核的原理
### 2.2.1 卷积核能够检测出特征的原理
1. 通俗解释：在图像中，相对于背景，描述物体的特征的像素之间的值差距较大(比如物体的轮廓)，变化明显，**通过卷积核过滤掉变化不明现的信息(背景信息)**
> 概念理解，原理呢？
2. 使用不同的卷积核来探测图像的不同的特征

### 2.2.2 在图像中的应用
1. 把图像相邻子区域的像素值和卷积核执行卷积操作，**可以获取相邻数据之间的统计关系，从而挖掘出图像中的某些重要特征，提供学习算法的健壮性(鲁棒性)**

## 2.3 如何选择卷积核
1. 卷积核是**超参数，需要人为的进行选择，是人们长期摸索而形成的先验知识**，常用的卷积核有以下几种(都是3x3的矩阵)：

### 2.3.1 同一化核(Identity)
啥都没做，卷积后得到的图像和原图一样==>因为这个核只有中心为1，其他都是0，因此滤波后的权值没有任何变化
```
000
010
000
```

### 2.3.2 边缘检测核(Edge Detetction)
高斯-拉普拉斯算子==>矩阵总和为0，**滤波后的图像除了边缘位置是亮的，其他位置都很暗**
```
-1-1-1
-1 8-1
-1-1-1
```
### 2.3.3 图像锐化核(Sharpness Filter)
和边缘检测类似，首先找到边缘，然后再把边缘加到原来的图像上，如此一来，强化了图像的边缘，使得图像看起来更加锐利
```
0 -1 0
-1 5 -1
0 -1 0
```

### 2.3.4 均值模糊核(Box Blur/Averaging)
把当前像素和它四邻域的像素一起取平均值，然后除以9
```
1/9 * 1 1 1
      1 1 1
      1 1 1
```

所谓的卷积核，就是一个权值矩阵，**主要用来处理单个像素和其他相邻像素之间的关系**
1. 如果卷积核中的各个权值相差较小，实际上就相当于每个像素和其他的像素取了平均值，因此有模糊降噪的功效
2. 如果相差较大(以卷积核中央元素来观察它与周边元素的差值)，就能拉大每个像素和周围像素的差距==>从而达到了提取图像中的物体边缘和锐化的效果

## 2.4 卷积层设计动机
### 2.4.1 生物学模拟的可行性
1. 不同的神经元，提取不同的特征图谱
2. 进行有机汇总

### 2.4.2 减少参数的必要性
1. 参数过多，会使得算力难以达到
2. 参数过多，会导致过拟合，因为不重要的参数，也会影响网络的输出结果

### 2.4.3 提取深层特征的重要性
通过调整**卷积核的参数**，提取不同的特质信息，将多个不同的卷积核进行组合，进一步提高网络的抽象能力(表征能力)

## 2.5 卷积层的特性===>卷积神经网络的核心所在
三个设计理念：局部连接，空间位置排列，权值共享

### 2.5.1 局部连接==>对应于特征提取
1. 每层的神经元，可以分布在**三维空间的每一个维度，长，宽，高**，三者相差就是单层神经元的总个数(长 * 宽 * 高)
2. 在全连接的情况下，第n层的神经元要和n-1层的每个神经元都有连接，这样会导致整个权值的总数是巨大的
3. 为了解决上述问题，通过局部连接(又称为稀疏连接)，也就是**隐含层的某个神经元只需要和前层部分区域相连接，这个局部连接区域称为感知域**==>大小和卷积核的大小相等==>**如何确定感知域?**
4. 卷积核的长度和宽度是相对原对象而言的，但是卷积层的高度(深度)是根据原图像的特征来决定的，因此要和原图像一致==>实际上也可以理解为是卷积核的个数，因为一个特征对应一个卷积核?

### 2.5.2 空间位置排列
卷积层的空间位置排列的4个参数：卷积核的大小 * 深度 * 步幅 * 补0

#### 大小
在前一节已经解释过了卷积核的大小

#### 深度
对应的是卷积核的个数，由于单个卷积核提取的特征常常是片面的，因此用多个卷积核来提取多个维度的特征===>是不是可以对应股票的各项指标？还是说这种网络不适合

#### 步幅
在输入矩阵上，滑动滤波矩阵的像素的单元个数s==>s越大，特征图会越小

#### 补0
当卷积核的大小不能被输入矩阵的维度大小整除时，会出现卷积核无法完全覆盖输入矩阵边界元素的情况，这些边界元素会无法参与卷积运算，此时通常有两种处理方式：
1. 不填充，去掉余数部分==>这种称为有效填充，这种填充的情况下，所处理的图像都是有效的，但是被迫做了裁剪，变小了
```
H(out) = H(in) - H(kernel) + 1;
W(out) = W(in) - W(kernel) + 1;
```
2. 等大填充，即在输入矩阵的周围填充若干圈合适的值，使得输入矩阵边界和卷积核的大小匹配，从而保证输出图片和原图片的大小保持一致，而所谓的合适的值，有两类:
- 填充最临近的边缘的像素值，即就地取材，重复利用，或者认为图片是无限循环的，用镜像翻转图片作为填充值==>**严格卷积**
- 直接填充0，即零值填充==>相当于对输入图像矩阵的边缘进行了一次滤波==>**泛卷积，应用更加广泛**

### 2.5.3 权值共享==>对应于降维处理
1. 权值实际上是**相邻两层不同神经元之间的连接参数**，因此也称为参数共享
2. 每个卷积核可以看作是一种特征提取方式，这种方式和图像的位置无关==>隐含假设是图像的统计特性和其他部分是一样的==>对于同一个卷积核，它在一个区域提取到的特征，也适用于其他的区域
3. 权值共享保证了在卷积时只需要学习一个参数集合即可，而不是针对每个位置都再学习一个单独的参数集合==>参数共享，也被称为绑定的权值

## 2.6 全连接，局部连接，权值共享
TBD==>需要进一步的研究

------
# 3. 激活层
1. 通过非线性的激活函数，来增强网络的表征能力，通常使用ReLU(修正线性单元)==>因为它收敛块，并且部分解决了梯度消失的问题
2. 神经网络分为显层和隐层，其中显层就是输入和输出，能够看到的；无法被看到的称为隐含层，它**可以理解为数据的内在表达**

## 3.1 通用近似定理
1. 如果隐含层的神经元足够多，那么神经网络就可以以任意精度逼近任意复杂度的连续函数==>**但是因为神经元之间的连接都是基于权值的线性组合，而线性的组合依然是线性的，因此网络的表达能力就会收到限制**
2. 为了解决上述问题，引入了**激活函数**，由于激活函数不一定是线性的，因此有了非线性的激活层，不管多么精妙的函数都能够近似表达出来===> 加入非线性的激活函数以后，深度神经网络才具有了分分层的非线性映射学习能力==>因此激活层是深度神经网络种不可或缺的重要部分

## 3.2 ReLU理论
1. 激活层存在的最大目的在于引入非线性因素，以增加整个网络的表征能力==>激活层是使用激活函数，常用的激活函数有Sigmoid/Tanh/ReLU/Softplus函数
2. ReLU函数相对于Sigmoid函数的优点在于：
- 单侧抑制
- 相对宽阔的兴奋边界
- 稀疏激活性
3. ReLU函数的缺点在于：
- 完全不限制，会导致神经元的输出永远为0的情况 

**TBD==>要理解一下这些内容**

------
# 4. 池化层
又叫亚采样层，即降低数据规模，**巧妙的采用，还具备局部线性转换不变性**，增强了网络的泛化处理能力==>**当卷积层提取目标的某个特征之后，通常会在两个卷积层中安排一个池化层**
> 局部线性转换不变性是指?

## 4.1 采样
1. 过多的训练数据量会导致计算能力加大，并且产生过拟合线性，因此采样用来减少训练数据量==>**其本质就是用合理的方式挑出典型**==>**在卷积神经网络中，采样主要是针对若干相邻的神经元而言的，因此也称作亚采样**==>**也就是池化，从一个池(一个区域)中提取出一个典型的代表来表征整体**

## 4.2 池化过滤器
1. 池化就是把小区域的特征通过整合得到新特征的过程，假设池化过滤器大小为w * h
2. 池化考察的是在w * h的区域内，所有元素具有某个一个特性==>常见的统计特性有**最大值，最小值，累加，L2泛数等**
3. 池化层函数力图**用统计特性反映出来的一个值，代替之前的w * h的整个子区域**
4. 池化层通过降低下一层待处理的数据量==>来减少参数的数量==>从而预防过拟合问题

## 4.3 池化函数
1. 最大池化函数
- 前向传播中，取最大值作为输出结果==>forward(1,2,3,2) = 3
- 反向传播中，最大值不变，其他的元素设置为0==>backward(3)=[0,0,3,0]

2. 平均池化函数
- 前向传播中，取平均值作为输出结果==>forward(1,2,3,2) = 2
- 反向传播中，所有的元素设置为平均值==>backward(2)=[2,2,2,2]

### 卷积和池化是为了突出实体的特征，并非为了提升图像的清晰度，因此通过池化操作后，原始图片像打了一层马赛克==>根据局部线性变换不变性原理，池化后的图片，不会影响它们的特征提取

## 4.4 局部线性变换不变性
1. 如果输入数据的局部进行了线性变化操作(如平移或者旋转)，那么经过池化操作后，输出结果并不会发生变化===>使得只用关心图像的特征，而不用关心图像的位置
2. 因为池化综合了(池化核范围内的)全部邻居的反馈==>通过k个像素统计特性，而不是单个像素提取特征==>**从而增强了神经网络的健壮性**

------
# 5. 全连接层 ==> 这是一个多层概念，不是只代表一层
1. 相当于多层感知机(MLP)，在卷积神经网络中起到了分类器的作用，经过**卷积->激活->池话**多层处理以后，得到的特征已经是具有代表性的了，因此能够得到成功率更高的神经网络
2. 卷积神经网络在全连接层使用的激活函数和BP不同==>常用的softmax函数
3. 全连接层的目的是把前面各层提取到的特征表示，映射到样本标记空间，**通过损失函数来调控学习过程，最后给出对象分类预测**

### 需要强调的是：在前面各层的多维拓扑结构在进入全连接层前会被拉平成一维，以便和后面的全连接层进行适配，这个维度变化有时候也被称为平坦层(Flatten Layer)==>它会称为全连接层的输入层==>其后面的网络拓扑结构和普通神经网络类似==>由若干隐含层和一个输出层

**全连接层虽然在卷积神经网络的最后面，但是它的参数占了网络的大部分==>容易陷入过拟合的问题，因此有时需要采用Droput的方式，随机丢掉部分节点来弱化过拟合现象**