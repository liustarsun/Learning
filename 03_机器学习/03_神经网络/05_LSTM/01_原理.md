# 1. 知识点

## 1.1 记忆和遗忘
1. 通常不仅需要记忆也需要遗忘，除了进行短期记忆以外，必要时还需要把**这些短期记忆拉长，留存下来，以备后用**
2. LSTM(Long Short-term Memory)长短期记忆，总的还是**短期记忆**，只是**有些短期记忆被保存下来了，历经的时序较长而已**

传统的RNN多采用的时BPTT算法，**这种算法的弊端在于，随着时间的流逝，网络层数增多，会产生梯度消失或梯度膨胀现象**
> 为什么时间流逝，网络层数增多，梯度消失/膨胀和激活函数有多大的关系？？**待研究**

## 1.2 梯度消失
1. 如果梯度较小(<1)，多层迭代后，**指数相乘(为何会有指数相乘？)**，梯度很快就会下降到对调整参数几乎没有影响，如(0.99)的100次方趋近于0==>**没有更好的解决方案**==>推动了LSTM网络结构的诞生
2. LSTM带有所谓的长短期记忆单元，由于其独特的设计结构，可以很好的解决梯度消失的问题==>**特别适合处理时序间隔和延迟非常长的任务(甚至超过1000个时间)，而且性能奇佳**==>非常适合构造大型的深度神经网络

## 1.3 梯度膨胀
1. 如果梯度较大(>1)，多层迭代后，又导致了梯度大的不得了，如(1.01)的100次方很大==>**解决方案：可以将其强制截断或者挤压**

------
# 2. LSTM分解
## 2.1 传统RNN的缺点
1. 对短期输入敏感，构建网络层数比较少时，能够有效的预测，但是当网络层数较深时，就会产生梯度弥散的问题
2. 过去有用但是有被抛弃的神经元，称为**泄漏单元**==>主要是记住了不该记住的，忘记了不该忘记的

## 2.2 改造神经元
1. LSTM的核心本质时：**通过引入巧妙的可控*自循环*，从而产生让梯度能够得以长时间可持续流动的路径**==>**实现是通过增加一个状态C，用来“合理的”保存长期的状态**
> 可控自循环??
2. 把新增加的状态C**称为记忆单元状态**，也称为**记忆块**，用来取代传统的隐藏层的神经元节点==>**负责把记忆信息从序列的初始位置传递到序列的末端**

```
# 隐藏层神经元的输入就变成了3个：
1. 当前时刻输入值X(t)
2. 前一个时刻的输出值S(t-1)
3. 前一时刻记忆单元的状态C(t-1)

# 隐藏层神经元的输出就变成了2个:
1. 当前时刻的输出值S(t)
2. 当前记忆单元的状态C(t)
```

## 2.3 记忆状态C和控制门开关
有效地控制记忆状态在LSTM中的实现是：通过设计3个控制门的开关(gate)==>打造一个可控记忆神经元
1. 开关1==>负责决定把前一个长期记忆C(t-1)在多大程度上保留到C(t)中==>**可选择性的遗忘部分之前积累的信息**
2. 开关2==>负责决定以多大程度把当前时刻的状态C'(t)保留到长期记忆状态C(t)中==>**这个不太理解C'(t)哪里来的？**
3. 开关3==>负责决定是否把长期状态C(t)作为当前LSTM的输出

## 2.4 LSTM的前向计算
所谓的门开关，在实现中就是一个**全连接网络层**==>**输入是一个复杂的矩阵向量，输出是一个0到1之间的实数向量(可以理解为一个连续的模拟数值)**==>模拟数值的优点是可导，适合反向传播调参==>LSTM就是通过调控某些全连接层的网络参数来达到调控输出的目的
```
1. x表示输入向量
2. W表示门的权重向量
3. b表示偏置向量

# 门的可用公式：
g(x) = f(W * X + b) ==> 激活函数f可以使用sigmoid函数的输出来控制门的开和关==>sigmoid函数的输出值被控制在0和1之间
1. 当输出为0时，任何向量和0相乘都是0， 相当于把门关上了
2. 当输出为1时，任何向量和1相乘都不会变，相当于门完全开启了
3. 当输出在0-1之间时，门相当于半开半掩，就可以调控记忆的存储程度
```
LSTM设计了两个门控制记忆单元C的信息量：
1. **遗忘门**==>决定上一时刻的单元状态有多少记忆可以保留到当前状态
2. **输入门**==>决定当前时刻的输入，有多少可以保留到单元状态C中==>**这个如何理解？？**
另外添加了额外两个门：
3. **候选门**==>控制以多大比例勾兑历史信息和当下的刺激
4. **输出门**==>用来控制单元状态有多少信息输出

## 2.5 LSTM中的门

### 2.5.1 遗忘门
遗忘门就是通过将**前一隐含层的输出S**(t-1)和**当前的输入X(t)进行线性组合**==>然后利用激活函数，把输出值压缩到0到1的区间之内==>输出值越靠近1，表明记忆块保留的信息越多==>反之保留的信息越少==>**哪里体现出记忆状态C？？**
```
f = g(W * S(t-1) + U * X(t) + b) ==>g用的时sigmoid
```

### 2.5.2 输入门
输入门的作用在于，它决定了**当前时刻下的输入信息X(t)**以多大程度添加到**记忆信息流中**，计算公式和遗忘门类似
```
i(t) = g(W * S(t-1) + U * X(t) + b) ===>g用的时sigmoid

```

### 2.5.3 候选门
可以理解为一个勾兑门==> 主要负责勾兑**当前输入信息**和**过去的记忆信息**==>**也就是负责计算当前输入的单元状态C'**==>只是使用的激活函数不同？
```
C'(t) = tanh(W * S(t-1) + U * X(t) + b) ===>激活函数用的是tanh==>把输出值规整到-1到1之间
```
然后需要把记忆状态从C(t-1)更新到C(t)，可以由两部分组成：
1. 通过遗忘门过滤掉不想保留的信息，大小记为：f(t) * C(t-1) ==> **通过遗忘门**
2. 通过输入门添加当前新增的信息，增加的比例由输入门控制，大小记为i(t) * C'(t) ==> **通过输入门**
3. 把上面的两个部分进行线性组合，得到更新后的记忆信息C(t)，即C(t) = f(t) * C(t-1) + i(t) * C'(t) ==> **输入门和输出门相互结合**

**总结：因为遗忘门的存在，它可以控制保留多久之间的信息；因为输入门的存在，可以避免把无关紧要的内容留在记忆中==>这样就把该忘记的忘记，该记住的记住，两者相得益彰**

### 2.5.4 输出门
内部的记忆状态更新完毕以后，然后就是决定要不要输出==>**输出门的作用在于：它控制着有多少记忆可以用在下一层网络的更新中**==>**下一层网络？？？**
```
O(t) = g(W * S(t-1) + U * X(t) + b) ==> 激活函数又用回了sigmoid==>不能任性输出，还需要使用激活函数tanh把记忆值变换成-1到1之间的数 ==> 最后的输出公式为：
S(t) = O(t) * tanh(C(t)) ==> 最后一步为何这样做，并没有解释清楚？ 
```

## 2.6 LSTM的总结
LSTM中的记忆单元状态，就像一条传送带一样，让信息量从记忆单元流过，其中做了一些线性的变化：包括了加法和乘法
1. 加法是LSTM的秘密所在==>它能够帮助LSTM在必须进行深度反向传播时，维持恒定的误差(或者说保留损失信号)==>这个损失信号正是调参的向导==>**这个也就是避免梯度消失的源头所在**
2. 乘法操作的前端使用sigmoid激活函数，因为函数输出在0到1之间，因此它代表了**信息保留的权重**

一加一乘，既克服了损失信号的弥散，又控制了信息保存的比例，是LSTM单元的精华所在

## 2.7 GRU(Gated Recurrent Unit)==>门控循环单元
一种简化的LSTM，主要是做了以下的简化
1. 提出了更新门的概念，把输入门和遗忘门合并，形成了一个新的更新门
2. 把记忆单元C(t)和隐含层单元S(t)进行了融合

-----
# 3. LSTM的训练过程
大致流程为：
1. 前向计算每个神经元的输出值==>计算出f(t), i(t), C(t), O(t)和S(t)
2. 确定损失函数
3. 根据损失函数梯度指引，更新网络权值参数==>误差的反向传播也包括两个层面：一是空间层面的，将误差项传到上一层网络；而是时间层面的，即从t时刻开始，计算每个时刻的误差

------
# 4. 自然语言处理

## 4.1 统计语言模型
基于预先收集的大规模语句库，以人类真实的语言为指导标准，利用统计手段预测文本序列在语句库中出现的概率，并以此概率作为评判文本是否合规的标准

## 4.2 词向量的表示方式
在自然语言处理中，词是最基本的单位，因此需要找到便于计算机理解的表示方式

### 4.2.1 独热编码表示
1. 对于n个单词，用1维大小为n的向量表示，给每个单词指定一个位置，除了它所在的位置以外，向量的其他位置都为0==>相似性是理解自然语言的重要方式，但是独热编码会导致神经网络缺失相似性的度量
2. 另外在海量的词典应用中，独热编码面临着巨大的维度灾难，已经在深度训练中的复杂的参数

### 4.2.2 分布式表示
1. 使用**一个连续的低维密集向量**来刻画一个词的特征==>从而可以直接刻画词与词之间的相似度，**并且可以构建一个从向量到概率的平滑函数模型，使得相似的词向量能够映射到相近的概率空间去**==> 这个稠密连续的向量也被称为**单词的分布式表示**
2. 所谓的分布式就是指**每个词都用一个向量表达，向量里面包含多个特征**
3. 分布式表示的最大贡献在于，它提供了一种可能性，可以让相关或者相似的词，在距离上可以度量==>度量的标准可以是欧氏距离，也可以是余弦夹角==>现在的需求则是需要把词特征找出来==>也就是使用**词嵌入的方法**

### 4.2.3 词嵌入表示==>(也叫做词向量)
1. 它能够从数据中自动学习到分布式表示==>从而显著的降低向量空间的维度及减少训练所需要的数据量
2. 词嵌入，从概念上讲，就是把一个维数等于所有词向量的高维空间(例如独热编码)，嵌入到一个维数低的多的连续向量空间中，并使得每个词或词组都被映射为实数域上的向量
3. 在数学上，嵌入表示一个映射==>也就是一个函数f，这个函数有点特殊，要满足两个条件：1. 单射，也就是每个Y只有唯一的与之对应；2. 结构保存，也即是在X所属空间上有x1 > x2，那么通过映射之后，在Y所属空间上一样有y1 > y2
4. 具体到词嵌入，也就是要找到一个映射或者函数，把词从高维空间映射到另外一个低维空间，而这个映射应该是满足**单射和结构保存两个条件** 

### 4.2.4 向量空间模型
#### 计数模型(LSA)
统计某个词出现的频率，把这些频率转换为小而密的矩阵

#### 预测模型
根据某个词周围的词，来推测这个词及其向量空间，相对于基于技术的模型，基于预测的模型有更多的超参数，更灵活一些

------
# 5. 自然语言处理的统计模型
如何计算一段文本序列在某种语言下出现的概率，是自然语言处理的统计模型的一个基本问题，也就是上下文语境问题，自然语言处理的统计模型的三个发展阶段：
1. NGram语言模型
2. 前馈神经网络模型(NNLM)
3. 循环神经网络模型(RNNLM)

## 5.1 NGram语言模型
### 5.1.1 马尔科夫假设
假设在一段文本钟，第n个单词出现的概率只和前面有限个n-1个词相关，而与其他词无关，通常n是远小于t的，即对句子进行了局部截断, n通常会选2，或者3，实验表明提高计算复杂度的4元模型，实际效果比3元模型更好

### 缺陷

## 5.2 前馈神经网络模型(NNLM)

## 5.3 循环神经网络模型(RNNLM)

## 5.4 正则化
正则化的目的是为了提高泛化能力，所要付出的代价就是降低了训练集的准确度
### 5.4.1 

### 5.4.2

### 5.4.3