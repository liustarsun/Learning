# 1. 知识点
## 1.1 沉没成本
有选择就有成本，没选择就没有成本==>当没有办法做选择时，就不存在成本一说==>沉没成本不是成本

## 1.2 马尔科夫链
简单来说，未来的一切只和现在相关，和历史无关==>但是人类的每一个决策，通常都是基于历史行为和当下状态叠加作用出来的

## 1.3 DBN(Deep Belief Network)，CNN(convolutional Neural Network)，RNN(Recurrent Neural Network)
深度信念网络，卷积神经网络和循环神经网络时深度学习的三大网络

## 1.4 DBN，CNN，等传统神经网络缺陷所在
传统神经网络的缺陷在于，它们的构建都基于这样一个假设，也就是训练集和测试集之间是相互独立的==>如果假设的本身就有问题，那么以此构建的模型自然也难以成立==>**DNN和CNN就很难在数据依赖的场景下胜出**；另外一个缺陷在于：标准神经网络的输入，都是标准的等长向量，比如输入层有10个节点，那么就只能接收10个元素，多了少了都不行
1. CNN擅长图像的识别，它使用池化策略时，能够带来空间平移不变性，借此CNN也提高了分类的鲁棒性
2. 但是文本数据和图像数据的区别相当大，因为文本对文字的前后顺序非常敏感
3. 并且文本数据的长度并不固定，可长可短，并且由于标准网络的各个输入之间是独立的，**因此无法感知相邻数据(或者说在时间序列数据)之间的依赖关系**

**以上的各种背景下，就产生了RNN循环神经网络**

------
# 2. 循环神经网络RNN
RNN的核心诉求之一，就是能把以往的信息连接到当前的任务之中

## 2.1 Hopfield网络
1. Hopfield网络是一种利用电阻，电容喝运算放大器实现的一个循环神经网络==>**它从输出到输入有反馈连接**
2. 引入能量函数的概念，从而把最优化问题的目标函数转换成了能量函数，通过网络能量函数最小化来寻找对应问题的最优解==>能量函数给判断网络运行稳定性提供了依据

## 2.2 Jordan递归神经网络
1. 一种前馈神经网络，**包含单个隐含层，输出节点将输出值反馈给一种特殊的单元**==>**上下文单元**==>下一时间步(下次计算时)，上下文单元负责把接收到的输出层的值，反馈给隐含层单元(和输入层的单元一起**作为隐含层的输入**)
2. 如果输出层的值是某种**行为**，那么特殊单元就允许记住前一个时间步发生的行为==>**如此同时，这些特殊单元还是自连接的**==>直观来看，这些边允许跨多个时间步发送信息，且不会干扰当前时间步的正常输出

## 2.3 Elman递归神经网络
1. 在网络中引入了循环，从而使得RNN具备有限短期记忆的优势，类似于Jordan递归神经网络，在Elman网络中，**每个隐含层的单元都配有上下文管理单元，可以理解为专配的秘书j'**==>每个秘书单元负责记录它的“主人单元j”==>也就是隐含层神经元j的前一个时间步的输出
2. 秘书上下文管理单元和主人单元的连接权重w(j'j)为1==>这也就意味着**秘书单元会作为一个普通的输入边，把接收到的前一个时间步的输出值，作为输入送回隐含层的单元**
> 为隐含层神经元配备固定权重的自连接循环构思，也是LSTM的重要理论基础==>**自连接的循环边，如何理解？？**

## 2.4 RNN应用领域
1. NLP(自然语言处理)==>两大经典问题**文本理解**和**文本生成**

------
# 3. RNN理论基础
## 3.1 循环
RNN之所以称为循环，是因为它的网络表现形式有循环结构==>使得过去的信息能够作为记忆被保存下来，并且应用于当前的输出计算之中

## 3.2 延迟连接
从上一个时刻的隐含状态s(t-1)到当前时刻隐含状态s(t)之间的连接

## 3.3 网络模型
1. 向量X==>输入层的值; 向量O==>输出层的值; 三类参数矩阵==>U(输入->隐含层权值),V(隐含层->输出层权值),W
2. 输入层神经元的个数n，隐含层神经元个数m，输出层神经元个数为r==>U(n * m)==>V(m * r)==>**前面的两个参数矩阵代表现在**；W代表的是用隐含层上一次的输出值来作为本次输入的权值矩阵(m * m)==>**代表历史**
3. 隐含层是RNN的核心，也是记忆信息的地方，不同类型的RNN，设计的区别就在于隐含层的设计不同上
> 通常还需要设置一个**初始隐含层**,表示初始的记忆状态，通常将其置为0
4. 在t时刻的记忆信息s(t)是由**前(t-1)个时间步累计而成的结果==>s(t-1)，当前的x(t)共同决定的**==>这些信息保存在隐含层，不断向后传递，跨越多个时间步==>共同影响每个新输入信息的处理结果

------
# 4. RNN的结构
针对不同的业务，做不同的区分，以下是以输入输出是否为固定长度来划分的

## 4.1 one to one
输入长度是固定的，输出也是固定的，也就是传统的网络==>网络的拓扑结构在设计的时候就是固定下来的

## 4.2 one to many
输入定长，输出变长的结构==>如给一个输入固定的词语“中国”，输出它的解释

## 4.3 many to one
输入变长，输出定长的结构==>比如给一段情绪的描述，输出是积极还是消极的概率

## 4.4 mang to many
分为两种，异步和同步的结构

### 4.4.1 异步
输出相对于输入，就类似于“流水线排空期”==>**也就是输出对于输入有延迟**
1. 编解码框架(Encoder-Decoder)：特点是把不定的输入序列经过编码以后，获得新的内部表示，然后基于这个表示进行解码，生成新的不定长的序列==>**典型的应用场景是机器翻译**
> 如翻译英文“I can not agree with you more”, 当**同步翻译到I can not agree with you(这段就类似于流水线排空期)时**，是“我不同意你的看法”，但是实际整句话的意思是“我非常同意你的看法”==>因此**翻译要有一定的滞后性(异步)来捕捉全句的意思**

### 4.4.2 同步
输入和输出对应，输入长度是可变的，输出也随着改变，且不存在输出延迟==>典型的应用场景是文本序列标注==>例如**可以利用RNN对给定文本的每个单词进行词性标注**


------
# 5. RNN的训练
1. RNN结构确定下来以后，接下来是进行训练，也就是**找到一个好的权值矩阵**==>换句话说，就是如何优化权值
2. 使用时间反向传播算法BPTT(BackPropagation Throug Time)来求解网络==>**核心任务是利用反向传播调参，从而使得损失函数最小**

## 5.1 问题建模
RNN的各种变种基本都是**集中在隐藏层**==>在Elman中，隐藏层的基本原理就是**把当前输入和反馈回来的记忆数据进行线性组合，以后利用非线性激活函数进行处理**
1. 确定损失函数的形式==>损失函数实际上就是衡量预期输出和实际输出的差异度函数==>作为教师信号，预期输出可以看作是常量==>因此第一步**分别确认隐藏层和输出层的输出函数(也就是激活函数)**
2. 隐藏层的激活函数常使用**Sigmoid/Tanh等**===>激活函数的作用常常比作**门电路**
```
s(t) = 0 (t=-1)
s(t) = sigmod(U*X + W*S(t-1) + b)
```
3. 输出层的激活函数使用softmax==> **需要注意的是输出层并不要求每个时间步都必须有输出==>它只需要给出整体的判定即可**
```
O(t) = V(t) * S(T) + c
```

## 5.2 确定优化目标函数
使用了交叉熵作为损失函数==>**和交叉熵相关的概念？**

## 5.3 参数求解
1. 利用梯度下降的方法来指导参数的迭代==>参数包含五类(V, c, W, U, b)
2. 因为激活函数使用的是sigmoid函数，因此每一层反向传播过程，梯度都会以前一层的1/4的速度递减==>随着传递时间步数的不断增加，梯度会**呈指数级减少的趋势，直到梯度消失**==>**一旦梯度消失，参数调整没了方向，BPTT就无法获得最优解==>RNN的应用就受到了局限性**=====>**这是激活函数的特性导致的**