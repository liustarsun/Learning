# 1. 梯度下降
## 1.1 导数
当函数定义域和取值都在实数域中的时候，导数可以表示**函数曲线上的切线斜率**。 除了切线的斜率，导数还表示函数在**该点的变化率**
> 导数代表了在自变量变化趋于无穷小的时候，函数值的变化与自变量变化的比值代表了导数，几何意义指该点的切线。物理意义指该时刻的(瞬时)变化率

## 1.2 偏导数
从导数到偏导数，即从曲线来到了曲面. 曲线上的一点，其切线只有一条。但是曲面的一点，切线有无数条，偏导数就是指的是多元函数沿坐标轴的变化率

### 1. 数据概念
- f{x}(x,y)指的是函数在y方向不变，函数值沿着x轴方向的变化率
- f{y}(x,y)指的是函数在x方向不变，函数值沿着y轴方向的变化率

### 2. 几何概念(_{0} 代表下标)
偏导数f_{x} (x_{0},y_{0} )就是曲面被平面y=y_{0}所截得的曲面在点M_{0}处的切线M_{0}T_{x}对x轴的斜率
偏导数f_{y} (x_{0},y_{0} )就是曲面被平面x=x_{0}所截得的曲面在点M_{0}处的切线M_{0}T_{y}对y轴的斜率

### 3. 局限性
偏导数指的是多元函数沿坐标轴的变化率，但是我们往往很多时候要考虑多元函数沿任意方向的变化率，那么就引出了方向导数

## 1.3 方向导数
使用x轴和y轴的偏微分可以求出任何方向的斜率(类似于一个平面的所有向量可以用俩个基向量来表示一样), 这这个方向上的极限就是方向导数，在这个点的向量就称为梯度，函数变化率最大的方向称为梯度的方向

## 1.4 梯度
梯度方向是函数局部上升最快的方向==>也就证明了**梯度的负方向是局部下降最快的方向**

## 1.5 梯度下降
迭代求解，每一次沿着**当前所处点的梯度负方向下降**，那么，最终会收敛到一个极小值

## 1.6 随机梯度下降SGD
所谓随机梯度下降，我们就是在计算每一轮的迭代值的时候，不需要用所有的样本点，随机选取其中一个就行

### 和梯度下降相比较的局限性
1. 梯度下降是每回按照迭代点所在等高线法向量方向前进，虽然计算的慢，收敛的慢，但是迭代的轮数比较少，走的很明确，而且最终会收敛到极小值点，凸函数的话会收敛到最小值点
2. 随机梯度下降每回走的就比较随意，迭代轮数比较多，但是有点就是算的快，收敛的快，但是缺点就是，最终收敛的地方，可能不是极小值点，可能收敛（这里我们定义的收敛就是前后两次迭代的差值小于某个上限）在最后一直在极小值附近的点

## 1.7 批量梯度下降BGD
从样本中选一批，也不像随机梯度下降一样选一个，取了一个二者折中的办法

------
# 2. 正则化
## 2.1 适定问题
满足以下三个条件的问题称为适定问题：
1. 解是存在的
2. 解释唯一的
3. 连续依赖于定解条件，即解是稳定的

## 2.2 不适定问题
不满足以上三个条件的问题，称为不适定问题

## 2.3 反问题
反问题通常体现了一种逆向思维，根据结果探求原因

## 2.4 正则化==>掌握机器学习中的概念==>用来处理过拟合问题
代数几何中的一个概念，在的损失函数的基础上，加上一些正则化项或者称为模型复杂度惩罚项

## 2.5 过拟合问题
往往源于特征过多，在训练集上的正确率很高，但是到预测集合性能很差，解决过拟合问题的方法一般有两种：
1. 减少特征的数量==>人工选择保留哪些特征；或者通过选择算法
2. 正则化==>保留所有特征，但是降低参数系数的值==>好处是当特征特别多时，每个特征都会对预测输出值贡献一份力量

正则化的一般方法是在损失函数后面加上一项，可以看做上损失函数的惩罚项，因为有公式，所以使用手写

## 2.6 L1正则化

## 2.7 L2正则化
------
# 3. 线性回归和逻辑回归


------
# 4. 极大似然估计